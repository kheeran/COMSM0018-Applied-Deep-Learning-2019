# -*- coding: utf-8 -*-
"""lab1-iris.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pfze8jfpXaYpq5V0BCk-pKq3ZugTyZI9
"""

import torch

from sklearn import datasets
iris = datasets.load_iris()

iris.keys()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import seaborn as sns
import pandas as pd

features_df = pd.DataFrame(iris['data'], columns= iris['feature_names'])
features_df['label'] = iris['target_names'][iris['target']]
# sns.pairplot(features_df, hue='label')

preprocessed_features = (iris['data'] - iris['data'].mean(axis=0)) / iris['data'].std(axis=0)

from sklearn.model_selection import train_test_split
labels = iris['target']
train_features, test_features, train_labels, test_labels = train_test_split(preprocessed_features, labels, test_size=1/3)

features = {'train': torch.tensor(train_features, dtype=torch.float32), 'test': torch.tensor(test_features, dtype=torch.float32)}
labels = {'train': torch.tensor(train_labels, dtype=torch.long), 'test': torch.tensor(test_labels, dtype=torch.long)}

from torch import nn
from torch.nn import functional as F
from typing import Callable

class MLP(nn.Module):
  def __init__(self, input_size:int, hidden_size: int, output_size: int, activation: Callable[[torch.Tensor], torch.Tensor] = F.relu):
    super().__init__()
    self.l1 = nn.Linear(input_size, hidden_size)
    self.l2 = nn.Linear(hidden_size, output_size)
    self.activation = activation

  def forward (self, inputs: torch.Tensor) -> torch.Tensor:
    x = self.l1(inputs)
    x = self.activation(x)
    x = self.l2(x)
    return x

input_size = 4
hidden_size = 100
output_size = 3
neural_net = MLP (input_size, hidden_size, output_size)

logits = neural_net.forward(features['train'])

# print (logits.size())

loss = nn.CrossEntropyLoss()

output = loss(logits, labels['train'])

# print (output)

output.backward()

# print (output.backward)

def accuracy (probs: torch.FloatTensor, targets: torch.LongTensor) -> float:
  trained_labels = torch.max (probs, 1)
  correct_labels = (targets == trained_labels.indices).sum()
  correct_labels_float = correct_labels.type(torch.FloatTensor)
  return torch.div (correct_labels_float, targets.size()[0])

# Testing accuracy function

'''
accuracy(logits, labels['train'])

def check_accuracy(probs: torch.FloatTensor,
                   labels: torch.LongTensor,
                   expected_accuracy: float):
    actual_accuracy = accuracy(probs, labels)
    assert actual_accuracy == expected_accuracy, f"Expected accuracy to be {expected_accuracy} but was {actual_accuracy}"

check_accuracy(torch.tensor([[0, 1],
                             [0, 1],
                             [0, 1],
                             [0, 1],
                             [0, 1]]),
               torch.ones(5, dtype=torch.long),
               1.0)
check_accuracy(torch.tensor([[1, 0],
                             [0, 1],
                             [0, 1],
                             [0, 1],
                             [0, 1]]),
               torch.ones(5, dtype=torch.long),
               0.8)
check_accuracy(torch.tensor([[1, 0],
                             [1, 0],
                             [0, 1],
                             [0, 1],
                             [0, 1]]),
               torch.ones(5, dtype=torch.long),
               0.6)
check_accuracy(torch.tensor([[1, 0],
                             [1, 0],
                             [1, 0],
                             [1, 0],
                             [1, 0]]),
               torch.ones(5, dtype=torch.long),
               0.0)
print("All test cases passed")
'''

from torch import optim
from torch.utils.tensorboard import SummaryWriter
import time

model = MLP(input_size, hidden_size, output_size)
optim_sgd = optim.SGD(model.parameters(), lr = 0.05)
loss_class = nn.CrossEntropyLoss()
summary_writer = SummaryWriter('logs', flush_secs=5)

if torch.cuda.is_available():
    DEVICE = torch.device("cuda")
    print ("Using CUDA...")
else:
    DEVICE = torch.device("cpu")
    print ("Using CPU...")

model = model.to(DEVICE)

features['train'] = features['train'].to(DEVICE)
features['test'] = features['test'].to(DEVICE)
labels['train'] = labels['train'].to(DEVICE)
labels['test'] = labels['test'].to(DEVICE)


start = time.time()

for epoch in range (0,100):
  forward_pass = model.forward(features['train'])
  loss = loss_class(forward_pass, labels['train'])
  train_accuracy = accuracy(forward_pass, labels['train'])*100

  summary_writer.add_scalar('accuracy/train', train_accuracy, epoch)
  summary_writer.add_scalar('loss/train', loss.item(), epoch)


  print ("epoch: {} - train-accuracy: {:2.1f}%, loss: {:5.5f}".format(epoch, train_accuracy, loss.item()) )
  loss.backward()
  optim_sgd.step()
  optim_sgd.zero_grad()

summary_writer.close()

forward_pass_test = model.forward(features['test'])
accuracy_test = accuracy(forward_pass_test, labels['test'])
print ("END - test accuracy: {:2.1f}%, time-taken: {:2.4f}".format(accuracy_test*100, time.time() - start))
